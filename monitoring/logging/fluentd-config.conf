# AtlasMesh Fleet OS - Fluentd Log Aggregation Configuration
# Comprehensive log collection for Abu Dhabi autonomous vehicle fleet operations
# Collects logs from all microservices, infrastructure, and edge devices

# Input Sources Configuration

# Kubernetes container logs
<source>
  @type tail
  @id input_tail_container_logs
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag raw.kubernetes.*
  read_from_head true
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </pattern>
    <pattern>
      format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    </pattern>
  </parse>
</source>

# AtlasMesh Fleet OS application logs
<source>
  @type tail
  @id input_tail_atlasmesh_logs
  path /var/log/atlasmesh/*.log
  pos_file /var/log/fluentd-atlasmesh.log.pos
  tag atlasmesh.*
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Vehicle telemetry logs (from edge devices)
<source>
  @type forward
  @id input_forward_vehicle_telemetry
  port 24224
  bind 0.0.0.0
  tag vehicle.telemetry
  <security>
    self_hostname fluentd.atlasmesh.local
    shared_key "#{ENV['FLUENTD_SHARED_KEY']}"
  </security>
</source>

# System logs (syslog)
<source>
  @type syslog
  @id input_syslog
  port 5140
  bind 0.0.0.0
  tag system.syslog
  <parse>
    message_format rfc3164
    with_priority true
  </parse>
</source>

# HTTP input for webhook logs
<source>
  @type http
  @id input_http_webhooks
  port 9880
  bind 0.0.0.0
  tag webhook
  body_size_limit 32m
  keepalive_timeout 10s
  <security>
    # Add authentication if needed
  </security>
</source>

# Prometheus metrics logs
<source>
  @type prometheus_monitor
  @id input_prometheus_monitor
  <labels>
    host #{Socket.gethostname}
    region abu_dhabi
    environment production
  </labels>
</source>

# Filter Configuration

# Parse Kubernetes logs
<filter raw.kubernetes.**>
  @type kubernetes_metadata
  @id filter_kubernetes_metadata
  kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
  verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
  ca_file "#{ENV['KUBERNETES_CA_FILE']}"
  skip_labels false
  skip_container_metadata false
  skip_master_url false
  skip_namespace_metadata false
  watch true
  de_dot false
  use_journal false
  merge_json_log true
  preserve_json_key_name true
  json_key_name log
  annotation_match ["fluentd.io/*"]
</filter>

# Filter AtlasMesh Fleet OS logs
<filter atlasmesh.**>
  @type record_transformer
  @id filter_atlasmesh_enrichment
  enable_ruby true
  <record>
    hostname "#{Socket.gethostname}"
    region abu_dhabi
    country uae
    timezone Asia/Dubai
    fleet_operator atlasmesh
    log_type application
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
    service_version "#{ENV['SERVICE_VERSION'] || '1.0.0'}"
  </record>
</filter>

# Filter vehicle telemetry logs
<filter vehicle.telemetry>
  @type record_transformer
  @id filter_vehicle_telemetry
  enable_ruby true
  <record>
    log_type vehicle_telemetry
    region abu_dhabi
    fleet_operator atlasmesh
    processed_at ${Time.now.utc.iso8601}
  </record>
</filter>

# Parse and enrich system logs
<filter system.syslog>
  @type record_transformer
  @id filter_system_logs
  <record>
    log_type system
    region abu_dhabi
    environment "#{ENV['ENVIRONMENT'] || 'production'}"
  </record>
</filter>

# Filter webhook logs
<filter webhook>
  @type record_transformer
  @id filter_webhook_logs
  <record>
    log_type webhook
    region abu_dhabi
    received_at ${Time.now.utc.iso8601}
  </record>
</filter>

# Security and PII filtering
<filter **>
  @type grep
  @id filter_security_exclusions
  <exclude>
    key log
    pattern /password|secret|token|key|credential/i
  </exclude>
</filter>

# PII redaction filter
<filter **>
  @type record_modifier
  @id filter_pii_redaction
  <record>
    log ${record["log"].to_s.gsub(/\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b/, '[REDACTED-CARD]').gsub(/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/, '[REDACTED-EMAIL]').gsub(/\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/, '[REDACTED-PHONE]')}
  </record>
</filter>

# Log level standardization
<filter **>
  @type record_transformer
  @id filter_log_level_standardization
  enable_ruby true
  <record>
    level ${record["level"] ? record["level"].upcase : (record["severity"] ? record["severity"].upcase : "INFO")}
  </record>
</filter>

# Abu Dhabi specific geolocation enrichment
<filter **>
  @type record_transformer
  @id filter_abu_dhabi_geolocation
  <record>
    geo_country UAE
    geo_region "Abu Dhabi"
    geo_city "Abu Dhabi"
    geo_coordinates "24.4539,54.3773"
    geo_timezone "Asia/Dubai"
  </record>
</filter>

# Fleet operation context enrichment
<filter atlasmesh.**>
  @type record_transformer
  @id filter_fleet_context
  enable_ruby true
  <record>
    fleet_context ${
      context = {}
      if record["service_name"]
        case record["service_name"]
        when "vehicle-gateway"
          context["operation_type"] = "vehicle_communication"
          context["criticality"] = "high"
        when "policy-engine"
          context["operation_type"] = "safety_decision"
          context["criticality"] = "critical"
        when "fleet-manager"
          context["operation_type"] = "fleet_coordination"
          context["criticality"] = "high"
        when "telemetry-ingestion"
          context["operation_type"] = "data_collection"
          context["criticality"] = "medium"
        end
      end
      context.to_json
    }
  </record>
</filter>

# Performance and latency tracking
<filter **>
  @type record_transformer
  @id filter_performance_tracking
  enable_ruby true
  <record>
    processing_latency_ms ${((Time.now.to_f - Time.parse(record["timestamp"] || record["time"] || Time.now.to_s).to_f) * 1000).round(2) rescue 0}
  </record>
</filter>

# Output Configuration

# Elasticsearch output for centralized logging
<match atlasmesh.**>
  @type elasticsearch
  @id output_elasticsearch_atlasmesh
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER'] || 'fluentd'}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || 'changeme'}"
  
  # Index configuration
  index_name atlasmesh-logs
  index_date_format %Y.%m.%d
  type_name _doc
  
  # Template configuration
  template_name atlasmesh-logs-template
  template_file /etc/fluentd/templates/atlasmesh-logs-template.json
  template_overwrite true
  
  # ILM configuration
  ilm_policy_id atlasmesh-logs-policy
  ilm_policy_overwrite true
  ilm_policy {
    "policy": {
      "phases": {
        "hot": {
          "actions": {
            "rollover": {
              "max_size": "10GB",
              "max_age": "1d"
            }
          }
        },
        "warm": {
          "min_age": "7d",
          "actions": {
            "allocate": {
              "number_of_replicas": 0
            }
          }
        },
        "cold": {
          "min_age": "30d",
          "actions": {
            "allocate": {
              "number_of_replicas": 0
            }
          }
        },
        "delete": {
          "min_age": "90d"
        }
      }
    }
  }
  
  # Buffer configuration
  <buffer>
    @type file
    path /var/log/fluentd-buffers/atlasmesh
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever true
    retry_max_interval 30
    chunk_limit_size 2MB
    queue_limit_length 8
    overflow_action block
  </buffer>
  
  # Performance tuning
  bulk_message_request_threshold 1048576
  flush_thread_count 4
  reload_connections false
  reconnect_on_error true
  reload_on_failure true
  log_es_400_reason true
</match>

# Vehicle telemetry to specialized index
<match vehicle.telemetry>
  @type elasticsearch
  @id output_elasticsearch_telemetry
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER'] || 'fluentd'}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || 'changeme'}"
  
  # Index configuration for telemetry
  index_name vehicle-telemetry
  index_date_format %Y.%m.%d
  type_name _doc
  
  # High-performance buffer for telemetry
  <buffer>
    @type file
    path /var/log/fluentd-buffers/telemetry
    flush_mode interval
    flush_interval 1s
    chunk_limit_size 5MB
    queue_limit_length 16
    flush_thread_count 8
    retry_type exponential_backoff
    retry_forever true
  </buffer>
</match>

# System logs to separate index
<match system.**>
  @type elasticsearch
  @id output_elasticsearch_system
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER'] || 'fluentd'}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || 'changeme'}"
  
  index_name system-logs
  index_date_format %Y.%m.%d
  type_name _doc
  
  <buffer>
    @type file
    path /var/log/fluentd-buffers/system
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 1MB
    queue_limit_length 4
  </buffer>
</match>

# Kubernetes logs
<match kubernetes.**>
  @type elasticsearch
  @id output_elasticsearch_kubernetes
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER'] || 'fluentd'}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || 'changeme'}"
  
  index_name kubernetes-logs
  index_date_format %Y.%m.%d
  type_name _doc
  
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 2MB
    queue_limit_length 8
  </buffer>
</match>

# Webhook logs
<match webhook>
  @type elasticsearch
  @id output_elasticsearch_webhooks
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER'] || 'fluentd'}"
  password "#{ENV['ELASTICSEARCH_PASSWORD'] || 'changeme'}"
  
  index_name webhook-logs
  index_date_format %Y.%m.%d
  type_name _doc
  
  <buffer>
    @type file
    path /var/log/fluentd-buffers/webhooks
    flush_mode interval
    flush_interval 5s
    chunk_limit_size 1MB
    queue_limit_length 4
  </buffer>
</match>

# Forward critical logs to external SIEM (if configured)
<match atlasmesh.** vehicle.telemetry>
  @type copy
  <store>
    @type forward
    @id output_forward_siem
    <server>
      name siem-server
      host "#{ENV['SIEM_HOST']}"
      port "#{ENV['SIEM_PORT'] || '24224'}"
      weight 60
    </server>
    <security>
      self_hostname fluentd.atlasmesh.local
      shared_key "#{ENV['SIEM_SHARED_KEY']}"
    </security>
    <buffer>
      @type file
      path /var/log/fluentd-buffers/siem
      flush_mode interval
      flush_interval 5s
      retry_type exponential_backoff
      retry_forever true
    </buffer>
  </store>
</match>

# Alerting output for critical events
<match **>
  @type copy
  <store>
    @type grep
    @id output_critical_alerts
    <regexp>
      key level
      pattern /ERROR|CRITICAL|FATAL/
    </regexp>
  </store>
  <store>
    @type webhook
    @id output_webhook_alerts
    endpoint "#{ENV['ALERT_WEBHOOK_URL']}"
    http_method post
    serializer json
    rate_limit_msec 1000
    <buffer>
      flush_mode immediate
    </buffer>
  </store>
</match>

# Prometheus metrics output
<match fluent.**>
  @type prometheus
  @id output_prometheus_metrics
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
      region abu_dhabi
    </labels>
  </metric>
  
  <metric>
    name fluentd_output_status_num_records_total
    type counter
    desc The total number of outgoing records
    <labels>
      tag ${tag}
      hostname ${hostname}
      region abu_dhabi
    </labels>
  </metric>
</match>

# Fallback output for unmatched logs
<match **>
  @type file
  @id output_file_fallback
  path /var/log/fluentd-fallback/unmatched.log
  append true
  time_slice_format %Y%m%d
  time_slice_wait 10m
  time_format %Y-%m-%dT%H:%M:%S%z
  <buffer>
    timekey 1h
    timekey_wait 10m
    flush_mode interval
    flush_interval 30s
  </buffer>
</match>

# Performance and monitoring
<system>
  # Fluentd system configuration
  workers "#{ENV['FLUENTD_WORKERS'] || '4'}"
  root_dir /var/log/fluentd
  log_level "#{ENV['FLUENTD_LOG_LEVEL'] || 'info'}"
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump true
  without_source true
  
  # Process monitoring
  <metrics>
    @type prometheus
    bind 0.0.0.0
    port 24231
    metrics_path /metrics
  </metrics>
  
  # Log rotation
  <log>
    format json
    time_format %Y-%m-%dT%H:%M:%S%z
    rotate_age 7
    rotate_size 100MB
  </log>
</system>
