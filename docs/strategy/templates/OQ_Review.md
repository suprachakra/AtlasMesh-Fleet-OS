# Outcome Quality (OQ) Review - [Feature Name]

**Feature**: [Feature name]  
**Release Date**: YYYY-MM-DD  
**Review Date**: YYYY-MM-DD (30 days post-launch)  
**Owner**: [PM Name]  
**Participants**: PM, Design, Eng Lead, Data Analyst

---

## 1. Original Hypothesis

### Problem We Set Out to Solve
[From original PRD: Problem statement]

### Expected Outcomes
| Metric | Baseline | Target | Expected Movement |
|--------|----------|--------|-------------------|
| [Metric 1] | [Value] | [Value] | [+/- %] |
| [Metric 2] | [Value] | [Value] | [+/- %] |

### Success Criteria
[From PRD: What constitutes success]

---

## 2. Actual Results

### Quantitative Outcomes

| Metric | Baseline | Target | Actual | Variance | Status |
|--------|----------|--------|--------|----------|--------|
| [Metric 1] | [Value] | [Value] | [Value] | [+/- %] | ‚úÖ/‚ö†Ô∏è/‚ùå |
| [Metric 2] | [Value] | [Value] | [Value] | [+/- %] | ‚úÖ/‚ö†Ô∏è/‚ùå |

**Dashboard Links**: [Links to metrics dashboards]

**Overall Outcome**: ‚úÖ Success | ‚ö†Ô∏è Partial Success | ‚ùå Did Not Meet Goals

### Qualitative Feedback

**User Satisfaction**:
- Survey responses: [#] responses, [average rating]
- Key positive feedback: [Themes]
- Key negative feedback: [Themes]
- Net Promoter Score (if applicable): [Score]

**Operational Impact**:
- Support ticket volume: [Baseline] ‚Üí [Actual]
- Common issues: [List]
- Training effectiveness: [Assessment]

---

## 3. Adoption & Usage

### Feature Adoption
- **Eligible Users**: [#] users/vehicles
- **Active Users**: [#] users/vehicles ([%] adoption rate)
- **Adoption Trend**: [Growing/Stable/Declining]

### Usage Patterns
- **Frequency**: [How often feature is used]
- **Completion Rate**: [% of initiated workflows completed]
- **Drop-off Points**: [Where users abandon]

**Telemetry Links**: [Feature usage dashboards]

---

## 4. Technical Performance

### SLI/SLO Compliance

| SLI | Target | Actual | Status |
|-----|--------|--------|--------|
| [SLI Name] | [Target] | [Actual] | ‚úÖ/‚ö†Ô∏è/‚ùå |

### Performance Issues
- **Incidents**: [#] P0, [#] P1, [#] P2
- **MTTR**: [Average time to resolve]
- **Error Rate**: [Baseline] ‚Üí [Actual]

### Reliability
- **Availability**: [Target] vs. [Actual]
- **Degradations**: [Count and duration]

---

## 5. Safety & Compliance

### Safety Impact
- **Assist Rate**: [Baseline] vs. [Actual]
- **Safety Incidents**: [#] incidents attributed to feature
- **Safety Gates**: All passing? ‚úÖ/‚ùå

### Compliance Status
- **Evidence Bundle**: Complete? ‚úÖ/‚ùå
- **Audit Findings**: [Any findings related to this feature]
- **Regulatory Feedback**: [Any regulator comments]

---

## 6. Variant Budget Impact

### Actual vs. Estimated Variant Cost

| Dimension | Estimated Delta | Actual Delta | Variance |
|-----------|-----------------|--------------|----------|
| Vehicle-Agnostic | [X%] | [Y%] | [+/- %] |
| Sector-Agnostic | [X%] | [Y%] | [+/- %] |
| Platform-Agnostic | [X%] | [Y%] | [+/- %] |
| Test Complexity | [X%] | [Y%] | [+/- %] |

**Variant Budget Status**: ‚úÖ Within Limits | ‚ö†Ô∏è At Limits | ‚ùå Exceeded

**Actions**:
- If exceeded: [Refactoring plan or CCB approval]

---

## 7. Learnings & Insights

### What Went Well
1. [Success #1]
2. [Success #2]
3. [Success #3]

### What Didn't Work
1. [Challenge #1 and why]
2. [Challenge #2 and why]
3. [Challenge #3 and why]

### Surprises (Unexpected Outcomes)
- [Unexpected positive outcome]
- [Unexpected negative outcome]
- [Unexpected user behavior]

### Key Insights for Future
1. [Insight that applies to future work]
2. [Process improvement]
3. [Technical learning]

---

## 8. Recommendations

### Continue, Iterate, or Kill

**Decision**: ‚úÖ Continue | üîÑ Iterate | ‚ùå Kill

**Rationale**: [Why this decision based on evidence]

### If Continue (As-Is)
- Monitor these metrics: [List]
- Review again in: [Timeframe]

### If Iterate (Improve)
**Iteration Plan**:
1. [Change #1 to make]
2. [Change #2 to make]
3. [Expected impact]

**Timeline**: [When iteration will be delivered]

### If Kill (Sunset)
**Deprecation Plan**:
- Follow deprecation playbook
- Timeline: [Proposed sunset date]
- Communication plan: [How we'll inform users]
- Fallback: [What customers should use instead]

---

## 9. Action Items

| Action | Owner | Due Date | Status |
|--------|-------|----------|--------|
| [Action item 1] | [Name] | YYYY-MM-DD | Open/Closed |
| [Action item 2] | [Name] | YYYY-MM-DD | Open/Closed |

---

## 10. Playbook Updates

### Updates to Make Based on This Review

**Discovery Playbook**:
- [Learning that should update discovery process]

**PRD Template**:
- [Learning that should update PRD template]

**Beta Program**:
- [Learning that should update beta process]

**This Playbook**:
- [Learning that should update OQ review process]

---

## 11. Approval & Sign-Off

| Role | Name | Date | Decision |
|------|------|------|----------|
| **PM Owner** | | | Continue/Iterate/Kill |
| **Eng Lead** | | | Agree/Disagree |
| **Design Lead** | | | Agree/Disagree |
| **CoP Steward** | | | Approved |

**Comments**: [Additional context or conditions]

---

## 12. Decision Record

**Documented in DACI Log**: DEC-YYYY-MM-###

**OKR Impact**: [Did this feature move the targeted KRs as expected?]

**Strategic Value**: [Confirmed/Needs Revision/Not Validated]

---

**This OQ review closes the product development loop by measuring actual outcomes vs. expectations and feeding learnings back into the product process.**

